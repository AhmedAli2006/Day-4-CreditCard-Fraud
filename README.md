# ğŸ§  Day 4 â€“ Credit Card Fraud Detection

### ğŸ“‚ Project Overview
This project detects fraudulent credit card transactions using real-world anonymized data.  
The goal is to accurately identify fraudulent activity from highly imbalanced datasets using advanced machine learning techniques.

---

## ğŸ¯ Objectives
- Understand and visualize the imbalance in transaction data  
- Apply feature scaling and preprocessing  
- Train and compare multiple classification models:
  - Logistic Regression  
  - Random Forest  
  - XGBoost  
- Handle class imbalance using weighting and SMOTE  
- Evaluate using precision, recall, F1-score, and ROC-AUC  
- Save the final trained model for deployment

---

## ğŸ§© Dataset
**Source:** [Kaggle â€“ Credit Card Fraud Detection Dataset](https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud)

- **Rows:** 284,807  
- **Features:** 30 (PCA-transformed for privacy)  
- **Target:**  
  - `Class = 0` â†’ Legitimate Transaction  
  - `Class = 1` â†’ Fraudulent Transaction  
- **Imbalance:** Only ~0.17% of all transactions are fraudulent.

---

## âš™ï¸ Tech Stack
- **Language:** Python  
- **Libraries:** pandas, numpy, scikit-learn, imbalanced-learn, seaborn, matplotlib, xgboost, joblib  
- **Tools:** Jupyter Notebook, GitHub  

---

## ğŸ“Š Exploratory Data Analysis
- Verified dataset shape and column details  
- Visualized severe class imbalance  
- Explored correlations of PCA components with target `Class`  
- Scaled `Time` and `Amount` columns using `StandardScaler`

---

## ğŸ§® Model Training & Evaluation

| Model | Precision (Fraud) | Recall (Fraud) | F1 | ROC-AUC | Notes |
|--------|------------------|----------------|----|----------|-------|
| Logistic Regression | 0.06 | 0.92 | 0.11 | 0.97 | High recall but many false positives |
| Random Forest | 0.96 | 0.75 | 0.85 | 0.96 | Balanced, strong generalization |
| **XGBoost** | **0.85** | **0.84** | **0.85** | **0.98** | âœ… Best trade-off between recall and precision |

**Key Insights:**
- Logistic Regression performed well in ranking (high AUC) but failed at precision.  
- Ensemble models (Random Forest, XGBoost) provided robust performance.  
- XGBoost achieved **ROC-AUC = 0.976**, catching most frauds with minimal false positives.

---

## ğŸ§  Feature Importance (XGBoost)
Top features contributing to fraud detection:
```
V14, V17, V10, V12, V4, V11, Amount
```
These represent principal components correlated with unusual transaction behavior.

---

## ğŸ§¾ Folder Structure
```
Day-4-CreditCard-Fraud/
â”œâ”€â”€ data/
â”‚   â””â”€â”€ creditcard.csv
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ fraud_detection.ipynb
â”œâ”€â”€ models/
â”‚   â””â”€â”€ fraud_detector_xgb.joblib
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## ğŸ’¾ Model Saving
The final trained model is stored as:
```
models/fraud_detector_xgb.joblib
```
It can be easily loaded for deployment:
```python
import joblib
model = joblib.load('models/fraud_detector_xgb.joblib')
```

---

## ğŸš€ Next Steps
- Integrate into a **FastAPI microservice** for real-time fraud detection  
- Build a **Streamlit dashboard** to visualize predictions  
- Deploy model to **AWS Lambda or Raspberry Pi** for edge inference  

---

## ğŸ§‘â€ğŸ’» Author
**Ahmed Ali**  
ğŸ“ Colchester, United Kingdom  
ğŸ”— [LinkedIn](https://www.linkedin.com/in/ahmed-ali2006/)  
ğŸ”— [GitHub](https://github.com/AhmedAli2006)

---
